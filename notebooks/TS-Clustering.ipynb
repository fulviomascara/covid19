{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste das bibliotecas de Clustering de Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "from dtaidistance import clustering\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Covid-19 EDA, master=local[*]) created by __init__ at <ipython-input-3-37d8f282b498>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-37d8f282b498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# criar contexto e configuração para o Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Covid-19 EDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/covid19/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/.virtualenvs/covid19/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Covid-19 EDA, master=local[*]) created by __init__ at <ipython-input-3-37d8f282b498>:3 "
     ]
    }
   ],
   "source": [
    "# criar contexto e configuração para o Spark\n",
    "conf = SparkConf().setAppName(\"Covid-19 EDA\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ler arquivo de casos mundiais confirmados para dataframe Spark\n",
    "confirmed_world_cases = sqlContext.read.csv('../datasets/time_series_covid19_confirmed_global.csv', sep=',', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte colunas de datas em linhas com melt\n",
    "confirmed_world_cases_pd = confirmed_world_cases.toPandas().melt(id_vars=[\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], var_name=\"date\", value_name=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte tipos das colunas para os formatos corretos\n",
    "\n",
    "confirmed_world_cases_pd.Lat = confirmed_world_cases_pd.Lat.astype(float)\n",
    "confirmed_world_cases_pd.Long = confirmed_world_cases_pd.Long.astype(float)\n",
    "confirmed_world_cases_pd.date = pd.to_datetime(confirmed_world_cases_pd.date)\n",
    "confirmed_world_cases_pd.Value = confirmed_world_cases_pd.Value.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recria Spark dataframes, com estrtutura nova\n",
    "schema_jhu = StructType([StructField('province_state', StringType(), True),\n",
    "                         StructField('country_region', StringType(), True),\n",
    "                         StructField('lat', FloatType(), True),\n",
    "                         StructField('long', FloatType(), True),\n",
    "                         StructField('date', TimestampType(), True),\n",
    "                         StructField('value', IntegerType(), True)])\n",
    "\n",
    "confirmed_world_cases = sqlContext.createDataFrame(confirmed_world_cases_pd, schema=schema_jhu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remover linhas onde value seja igual a zero\n",
    "confirmed_world_cases = confirmed_world_cases.filter(\"value != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar coluna com qtde_dias desde o ínicio, pra cada país\n",
    "column_list = ['province_state', 'country_region']\n",
    "\n",
    "confirmed_world_cases = confirmed_world_cases.select(\"province_state\",\"country_region\", \"lat\", \"long\", \"date\", \\\n",
    "                                             \"value\", F.row_number()\\\n",
    "                                             .over(Window.partitionBy([col(x) for x in column_list]) \\\n",
    "                                             .orderBy(confirmed_world_cases['date'])).alias(\"num_days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Janela de Dados por País, Província e Numero de Dias (poderia ser a data também)\n",
    "my_window = Window.partitionBy(\"country_region\", \"province_state\").orderBy(\"num_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar coluna para casos confirmados no dia\n",
    "confirmed_world_cases = confirmed_world_cases.withColumn(\"prev\", F.lag(confirmed_world_cases.value).over(my_window))\n",
    "confirmed_world_cases = confirmed_world_cases.withColumn(\"on_day\", F.when(F.isnull(confirmed_world_cases.value - confirmed_world_cases.prev), confirmed_world_cases.value)\n",
    "                              .otherwise(confirmed_world_cases.value - confirmed_world_cases.prev))\n",
    "\n",
    "# Remover as colunas de lag\n",
    "confirmed_world_cases = confirmed_world_cases.drop(*['prev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dataframe para clustering\n",
    "\n",
    "# remover colunas de province_state, lat, long, date\n",
    "columns_to_drop = ['province_state', 'lat', 'long', 'date']\n",
    "clustering_df = confirmed_world_cases.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+-----------------+------------------+\n",
      "|summary|country_region|            value|         num_days|            on_day|\n",
      "+-------+--------------+-----------------+-----------------+------------------+\n",
      "|  count|         18889|            18889|            18889|             18889|\n",
      "|   mean|          null|5867.937900365292|39.74816030493938| 217.1474932500397|\n",
      "| stddev|          null|43852.27773708838|26.28890594010864|1546.5062526308423|\n",
      "|    min|   Afghanistan|                1|                1|            -10034|\n",
      "|    25%|          null|               16|               18|                 0|\n",
      "|    50%|          null|              141|               36|                 2|\n",
      "|    75%|          null|              833|               56|                28|\n",
      "|    max|      Zimbabwe|          1329260|              110|             36188|\n",
      "+-------+--------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar um dataframe, convertendo num_days em colunas (pivot)\n",
    "clustering_pivot_df = clustering_df.groupby(\"country_region\").pivot(\"num_days\").sum(\"on_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar numpy array, com values\n",
    "clustering_array_np = np.array(clustering_pivot_df.select(clustering_pivot_df.columns[10:20]).limit(20).collect(), dtype=np.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_array_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    2.,    2.,    0.,    1.,    0.,    1.,    0.,\n",
       "           0.],\n",
       "       [   1.,    2.,    0.,    2.,    5.,    4.,    0.,    5.,   10.,\n",
       "           4.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    6.,    0.,   14.,    0.,    2.,    5.,    0.,\n",
       "           7.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   1.,    0.,    0.,    0.,    0.,    0.,    1.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,   12.,    1.,  -15.,    0.,    0.,    0.,    3.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    3.,    0.,    7.,    0.,    7.,    0.,    2.,    0.,\n",
       "           2.],\n",
       "       [   0.,    1.,    0.,    0.,    1.,    4.,   11.,    3.,   21.,\n",
       "           1.],\n",
       "       [   1.,    2.,    4.,    0.,   12.,    3.,    7.,    9.,    1.,\n",
       "           9.],\n",
       "       [   0.,    2.,    2.,    0.,    0.,    4.,    0.,    2.,    0.,\n",
       "           0.],\n",
       "       [   3.,    2.,    0.,    6.,    4.,    0.,    2.,    3.,    7.,\n",
       "           5.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    2.,    0.,    5.,    0.,\n",
       "           2.],\n",
       "       [ 167.,  311.,  566.,  293.,  343.,  561., 1196., 2069., 1704.,\n",
       "        1815.],\n",
       "       [   3.,    1.,    3.,    0.,    0.,    0.,    1.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   3.,    0.,    5.,   14.,    6.,    0.,   11.,    0.,    0.,\n",
       "          30.],\n",
       "       [   0.,    0.,    1.,    0.,    1.,    0.,    2.,    0.,    0.,\n",
       "           0.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imprimir array sem notação científica\n",
    "np.set_printoptions(suppress=True)\n",
    "clustering_array_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▌| 19/20 [00:00<00:00, 14032.71it/s]\n",
      "\n",
      " 95%|█████████▌| 19/20 [00:00<00:00, 12977.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Clustering\n",
    "model1 = clustering.Hierarchical(dtw.distance_matrix_fast, {})\n",
    "cluster_idx = model1.fit(clustering_array_np)\n",
    "# Augment Hierarchical object to keep track of the full tree\n",
    "model2 = clustering.HierarchicalTree(model1)\n",
    "cluster_idx = model2.fit(clustering_array_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciPy linkage clustering\n",
    "model3 = clustering.LinkageTree(dtw.distance_matrix_fast, {})\n",
    "cluster_idx = model3.fit(clustering_array_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.plot(\"plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
